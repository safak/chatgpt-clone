{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/madeeltariq/api-gateway-rag?scriptVersionId=214646487\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# *Installing the required libraries*# ","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport sys\nimport os\n\ndef install_package(package_name):\n    try:\n        # Check if the package is installed\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"show\", package_name])\n        print(f\"{package_name} is already installed.\")\n    except subprocess.CalledProcessError:\n        # If package is not installed, install it\n        print(f\"Installing {package_name}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n        print(f\"{package_name} installation completed.\")\n\ndef install_ngrok():\n    # Check if ngrok is already downloaded and extracted\n    if not os.path.exists('ngrok'):\n        print(\"Downloading and installing ngrok...\")\n        subprocess.run(['wget', '-q', '-O', 'ngrok.zip', 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip'])\n        subprocess.run(['unzip', '-o', 'ngrok.zip'])\n        print(\"ngrok installed successfully.\")\n    else:\n        print(\"ngrok is already installed.\")\n\n# Installing packages step-by-step\ninstall_package(\"flask\")\ninstall_package(\"transformers\")\ninstall_package(\"pinecone-client\")\ninstall_package(\"python-dotenv\")\ninstall_package(\"flask-cors\")\ninstall_package(\"pyngrok\")\n\n# Install ngrok separately\ninstall_ngrok()\n\nprint(\"Done With All installations >>>>>>>>>>>>>>>>> Move Forward\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports in here","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Code cell for Entering the ngrok API key For Ngrok implementation**","metadata":{}},{"cell_type":"code","source":"import os\nimport getpass  # Import the getpass module to hide input text\n\n# Function to create/update .env file with ngrok API key\ndef set_ngrok_api_key():\n    # Ask user for the ngrok API key without displaying the input\n    ngrok_api_key = getpass.getpass(\"Please enter your ngrok API key: \")\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the key; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"NGROK_AUTHTOKEN={ngrok_api_key}\\n\")\n    \n    print(f\"ngrok API key added to {env_file}\")\n\n# Function to create/update .env file with Pinecone credentials\ndef set_pinecone_credentials():\n    # Ask user for the Pinecone API key without displaying the input\n    pinecone_api_key = getpass.getpass(\"Please enter your Pinecone API key: \")\n\n    # Ask user for the Pinecone environment, with a default value\n    pinecone_env = input(\"Please enter your Pinecone environment (e.g., us-west1-gcp) [default: us-east-1]: \") or \"us-east-1\"\n    \n    # Check if .env file exists\n    env_file = '.env'\n    \n    # If file exists, append the keys; if not, create a new file\n    if os.path.exists(env_file):\n        with open(env_file, 'a') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    else:\n        with open(env_file, 'w') as file:\n            file.write(f\"PINECONE_API_KEY={pinecone_api_key}\\n\")\n            file.write(f\"PINECONE_ENV={pinecone_env}\\n\")\n    \n    print(f\"Pinecone credentials added to {env_file}\")\n\n# Call the functions to set the API key and credentials\nset_ngrok_api_key()\nset_pinecone_credentials()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv  # Import the dotenv package to load .env file\nfrom pinecone import Pinecone\n\n# Function to get vector from Pinecone database based on ID\ndef get_vector_from_pinecone(file_id, index_name=\"vecotr\", namespace_name=\"newCheck\", pinecone_env=\"us-east-1\"):\n    # Load environment variables from .env file\n    load_dotenv()  # This will load the variables from the .env file into the environment\n\n    # Load Pinecone API key from the environment variables\n    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n    if not pinecone_api_key:\n        print(\"Check if ENV variables are set\")\n\n    # Ensure the API key is not None\n    if not pinecone_api_key:\n        # print(\"Error: Pinecone API key is missing from the environment variables.\")\n        return None\n\n    # Initialize Pinecone client\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        print(f\"Pinecone client initialized with environment: {pinecone_env}\")\n    except Exception as e:\n        print(f\"Error initializing Pinecone client: {str(e)}\")\n        return None\n\n    # Initialize the index\n    try:\n        index = pc.Index(index_name)\n    except Exception as e:\n        print(f\"Error initializing Pinecone index: {str(e)}\")\n        return None\n\n    # Query Pinecone to retrieve vector for the specific ID\n    try:\n        print(f\"Retrieving vector for file ID: {file_id} from Pinecone...\")\n\n        # Querying Pinecone index for the specific ID using metadata filtering\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            id=file_id,\n            top_k=1,\n            include_values=True,\n            include_metadata=True\n        )\n\n        # Check if result contains matches\n        if response and response.get(\"matches\"):\n            # Display the first few vectors based on top_k\n            for idx, vector_data in enumerate(response[\"matches\"]):\n                # print(f\"Vector {idx + 1} ID: {vector_data['id']}\")\n                print(f\"Pinecone Index: {index_name}\")\n                print(f\"Pinecone Env: {pinecone_env}\")\n                # print(f\"Metadata (fileId): {vector_data['metadata']['fileId']}\")\n                # print(f\"Metadata (text): {vector_data['metadata']['text'][:100]}...\")  # Showing first 100 chars of text\n                print(f\"Vector Values: {vector_data['values'][:5]}...\")  # Showing first 5 values of the vector\n                print(f\"Size of Vector Values: {len(vector_data['values'])}\")  # Printing the size of the vector values\n            return response[\"matches\"]\n\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving vector for file ID {file_id}: {str(e)}\")\n        return None\n\n# Example usage of the function\nfile_id = \"676594520c3eb8c3272efa2c\"  # Replace with the file ID you want to query\n\nvector = get_vector_from_pinecone(file_id)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To test this one is ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom dotenv import load_dotenv\nimport os\nfrom pinecone import Pinecone\n\n# Load the environment variables from .env file\nload_dotenv()\n\n\n\n\n# Function to retrieve and compare query vector with Pinecone vectors using cosine similarity\ndef find_similar_vectors(query):\n    # Initialize the sentence transformer model that matches Pinecone's vector dimensionality (1024 in this case)\n    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # You can change this to a 1024-dimensional model\n    \n    # Vectorize the query\n    query_vector = model.encode([query])[0]  # Convert the query into a vector\n\n    # Fetch vector data from Pinecone for a sample file\n    file_id = \"676594520c3eb8c3272efa2c\"  # Replace with your actual file ID\n    result = get_vector_from_pinecone(file_id)\n\n    if result is None:\n        print(\"No results found.\")\n        return\n\n    # Extract the first match from the result\n    if isinstance(result, list) and len(result) > 0:\n        match = result[0]  # Get the first match from the list of results\n\n        # Extract the vector and metadata from the match\n        stored_vector = match['values']\n        metadata = match['metadata']\n\n        # Check if the query vector and the stored vector have the same dimensionality\n        if len(query_vector) != len(stored_vector):\n            print(f\"Error: Dimensionality mismatch! Query vector length: {len(query_vector)}, Stored vector length: {len(stored_vector)}\")\n            return\n\n        # Calculate cosine similarity between the query vector and the Pinecone vector\n        similarity = cosine_similarity([query_vector], [stored_vector])[0][0]\n\n        # Print the similarity and the metadata of the most similar vector\n        print(f\"Cosine Similarity: {similarity:.4f}\")\n        print(f\"Metadata (Text): {metadata.get('text', 'No text metadata available')}\")\n        print(f\"Stored Vector: {stored_vector[:5]}...\")  # Display first 5 values of the stored vector\n\n        return similarity, metadata\n    else:\n        print(\"No valid matches found in the result.\")\n        return None\n\n# Example usage\nquery = \"What is the university name that is teaching the compiler construction course?\"  # Example query\nsimilarity, metadata = find_similar_vectors(query)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\ndef get_response_from_llm(query, model_name=\"EleutherAI/gpt-neo-1.3B\"):\n    \"\"\"\n    This function takes a user query, processes it using a free LLM from Hugging Face, \n    and returns the generated response.\n\n    :param query: User input query.\n    :param model_name: Model name to be used for text generation.\n    :return: LLM's response to the query.\n    \"\"\"\n    # Load the model pipeline\n    generator = pipeline(\"text-generation\", model=model_name, device=-1)  # Use CPU (-1) or GPU (set device to appropriate value)\n\n    # Generate response\n    response = generator(query, max_new_tokens=100, do_sample=True)\n    \n    # Return the generated response\n    return response[0][\"generated_text\"]\n\n# Example usage\nuser_query = \"What is the significance of artificial intelligence?\"\nresponse = get_response_from_llm(user_query)\nprint(\"Response from the LLM:\", response)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implemented Ngrok and CORS on top of the Flask API","metadata":{}},{"cell_type":"code","source":"import os\nimport threading\nimport time\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS  # Import CORS to handle cross-origin requests\nfrom pyngrok import ngrok  # Import ngrok\nfrom dotenv import load_dotenv  # Import the dotenv package to load .env file\nfrom pinecone import Pinecone\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Function to get vector from Pinecone database based on query and file ID\ndef get_vector_from_pinecone(file_id, query, index_name=\"vecotr\", namespace_name=\"newCheck\", pinecone_env=\"us-east-1\"):\n    # Load Pinecone API key from the environment variables\n    pinecone_api_key = os.getenv('PINECONE_API_KEY')\n    if not pinecone_api_key:\n        print(\"Check if ENV variables are set\")\n        return None\n\n    # Initialize Pinecone client\n    try:\n        pc = Pinecone(api_key=pinecone_api_key)\n        index = pc.Index(index_name)\n    except Exception as e:\n        print(f\"Error initializing Pinecone client or index: {str(e)}\")\n        return None\n\n    # Embedding the query\n    try:\n        embedding_response = pc.inference.embed(\n            model=\"multilingual-e5-large\",\n            inputs=[query],\n            parameters={\n                \"input_type\": \"query\",\n                \"truncate\": \"END\"\n            }\n        )\n        if embedding_response and hasattr(embedding_response, 'data') and len(embedding_response.data) > 0:\n            query_embedding = embedding_response.data[0].get('values', [])\n            if not query_embedding:\n                print(\"Error: 'values' key is missing or empty in the embedding response.\")\n                return None\n        else:\n            print(\"Error generating embedding for the query. No data found.\")\n            return None\n    except Exception as e:\n        print(f\"Error generating embedding: {str(e)}\")\n        return None\n\n    # Query Pinecone\n    try:\n        response = index.query(\n            namespace=namespace_name,\n            filter={\"fileId\": {\"$eq\": file_id}},\n            vector=query_embedding,\n            top_k=5,\n            include_values=True,\n            include_metadata=True\n        )\n        if response and \"matches\" in response:\n            matches = response[\"matches\"]\n            threshold = 0.8\n            filtered_matches = [m for m in matches if m.get(\"score\", 0) >= threshold]\n            if not filtered_matches:\n                print(\"No matches above the similarity threshold.\")\n                return None\n            filtered_matches = sorted(filtered_matches, key=lambda x: x.get('score', 0), reverse=True)\n            return filtered_matches\n        else:\n            print(f\"No vector found for file ID: {file_id}\")\n            return None\n    except Exception as e:\n        print(f\"Error retrieving vector for file ID {file_id}: {str(e)}\")\n        return None\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app)  # Enable CORS for all routes\n\n@app.route('/get_model_response', methods=['POST'])\ndef get_model_response():\n    try:\n        data = request.get_json()\n        if not data:\n            return jsonify({\"error\": \"Request body must be JSON\"}), 400\n        file_id = data.get(\"file_id\")\n        query = data.get(\"query\")\n        if not file_id:\n            return jsonify({\"error\": \"'file_id' is required\"}), 400\n        vector_response = get_vector_from_pinecone(file_id, query)\n        \n        if vector_response:\n            enriched_matches = []\n            for vector_data in vector_response:\n                # Ensure vector_data is safely converted into a dictionary-like structure\n                if hasattr(vector_data, \"to_dict\"):\n                    vector_data = vector_data.to_dict()\n        \n                # Filter out unnecessary data and include similarity score\n                \n                filtered_data = {k: v for k, v in vector_data.items() if k != 'values'}\n                # filtered_data['similarity'] = vector_data.get('score', None)\n        \n                enriched_matches.append(filtered_data)\n        \n            return jsonify({\"status\": \"success\", \"data\": enriched_matches}), 200\n        else:\n            return jsonify({\"status\": \"error\", \"message\": \"No vector found for the given file_id\"}), 404\n\n    except Exception as e:\n        print(f\"[ERROR] Exception occurred: {str(e)}\")\n        return jsonify({\"status\": \"error\", \"message\": \"An error occurred\"}), 500\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\ndef run_flask():\n    app.run(host=\"0.0.0.0\", port=5000)\n\ndef run_ngrok():\n    ngrok_auth_token = os.getenv(\"NGROK_AUTHTOKEN\")\n    if ngrok_auth_token:\n        ngrok.set_auth_token(ngrok_auth_token)\n    else:\n        print(\"Ngrok authentication failed. Please check your API key.\")\n        return None\n    public_url = ngrok.connect(5000)\n    print(f\"Ngrok public URL: {public_url}\")\n    return public_url\n\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True\nflask_thread.start()\npublic_url = run_ngrok()\n\nwhile True:\n    time.sleep(1)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-24T20:44:39.321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Working Prototype for flask listening***","metadata":{}},{"cell_type":"code","source":"import os\nfrom flask import Flask, request, jsonify\nimport threading\nimport time\n\n# Initialize Flask app\napp = Flask(__name__)\n\n# Health check endpoint\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({\"status\": \"Flask server is running Greate!\"})\n\n@app.route('/test', methods=['GET'])\ndef test():\n    return jsonify({\"message\": \"Flask server is running!\"})\n\n# Function to run Flask server\ndef run_flask():\n    print(\"Flask server is listening on port 8000...\")\n    app.run(host=\"0.0.0.0\", port=9000)  # Running on port 8000\n\n# Start Flask server in a separate thread to keep the notebook running\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.daemon = True  # Allow thread to terminate when the program exits\nflask_thread.start()\n\n# Print out the URL where Flask is running\nprint(\"Flask server should be accessible at: http://0.0.0.0:8000\")\n\n# Keep the notebook running to maintain the server\nwhile True:\n    time.sleep(1)  # Just keeps the notebook running\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How to send request from the JavaScript Server Side ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}